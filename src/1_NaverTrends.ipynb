{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAVER TRENDS CRAWLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 공식 API 제공 (ID/SECRET 발급)\n",
    "* 각 영화마다 키워드 설정하여 트랜드 조회 가능\n",
    "* 기간별, 성별, 연령별, 채널별 조회 가능\n",
    "* 동시 조회 가능 검색어 5개\n",
    "\n",
    "[Reference](https://brunch.co.kr/@sukhyun9673/13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverTrend:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        NaverTrend Class\n",
    "        \"\"\"\n",
    "        self.min_movie = []\n",
    "        self.max_movie = []\n",
    "\n",
    "    def set_api_info(self, api_info):\n",
    "        self.client_id = api_info[\"client_id\"]\n",
    "        self.client_secret = api_info[\"client_secret\"]\n",
    "        self.url = api_info[\"url\"]\n",
    "        return\n",
    "\n",
    "    def load_data(self, movie_data):\n",
    "        self.movie_data_full = movie_data\n",
    "        self.movie_list_full = movie_data.iloc[:,0]\n",
    "        return\n",
    "\n",
    "    def _define_body(self, movie_list):\n",
    "        \"\"\"\n",
    "        movie_list: list\n",
    "        TREND 조회 요청에 필요한 Body 파트를 정의한다.\n",
    "         - 상세 조건 parameter로 받을 수 있도록 수정 필요\n",
    "         - movie_list, keyword_list 쌍으로 받을 수 있도록 수정 필요 (DataFrame, Nested List, Two Series, ...)\n",
    "        \"\"\"\n",
    "        keywords_list = [\"{\\\"groupName\\\": \\\"\" + movie + \"\\\", \\\"keywords\\\": [\\\"\" + movie + \"\\\"]}\" for movie in movie_list]\n",
    "        body =\"{\\\"startDate\\\":\\\"2018-12-01\\\", \\\"endDate\\\":\\\"2020-02-28\\\", \\\"timeUnit\\\":\\\"date\\\", \\\"keywordGroups\\\":\" + \"[\" + \", \".join(keywords_list) + \"]}\"\n",
    "        return(body)\n",
    "\n",
    "    def _get_response_body(self, body):\n",
    "        \"\"\"\n",
    "        body: string generated by _define body()\n",
    "        Naver API로부터 트렌드 조회 결과를 받아온다.\n",
    "        \"\"\"\n",
    "        request = urllib.request.Request(self.url)\n",
    "        request.add_header(\"X-Naver-Client-Id\", self.client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", self.client_secret)\n",
    "        request.add_header(\"Content-Type\", \"application/json\")\n",
    "        response = urllib.request.urlopen(request, data = body.encode(\"utf-8\"))\n",
    "        rescode = response.getcode()\n",
    "        if(rescode==200):\n",
    "            response_str = response.read().decode(\"utf-8\") # repsonse body string\n",
    "            response_dict = json.loads(response_str) # response body dict\n",
    "            return(response_dict)\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "    \n",
    "    def _response_to_df(self, response_dict):\n",
    "        \"\"\"\n",
    "        response_dict: dictionary generated by _get_response_body()\n",
    "        response_body_dict를 DataFrame으로 변환한다.\n",
    "        \"\"\"\n",
    "        trend_list = []\n",
    "        for i in range(len(response_dict[\"results\"])): # Need to find for loop alternatives.\n",
    "            ttl = response_dict[\"results\"][i][\"title\"]\n",
    "            data = response_dict[\"results\"][i][\"data\"]\n",
    "            \n",
    "            time = np.array([datetime.strptime(i[\"period\"], \"%Y-%m-%d\") for i in data])\n",
    "            value = np.array([j[\"ratio\"] for j in data])\n",
    "            title = np.repeat(ttl, len(value), axis=0)\n",
    "            \n",
    "            trend = pd.DataFrame({\"Keyword\": title, \"Time\": time, \"TrendIndex\": value})\n",
    "            trend_list.append(trend)\n",
    "        response_df = pd.concat(trend_list).reset_index(drop=True)\n",
    "        return(response_df)\n",
    "\n",
    "    def _recursive_search(self, init_cand, find_max=True):\n",
    "        \"\"\"\n",
    "        init_cand: initial min/max candidates\n",
    "        전체 리스트 중 검색 횟수가 최대/최소인 영화를 찾을 때까지 5개씩 TREND를 조회한다.\n",
    "        \"\"\"\n",
    "        def _search(cand):\n",
    "            n_movie = len(cand)\n",
    "            rng = range(0, n_movie, 5)\n",
    "\n",
    "            cand_subset = []\n",
    "            for i, idx in zip(rng, range(len(rng))):\n",
    "                movie_subset = [*cand[i:i+5]]\n",
    "                trend_subset = self.get_trend(movie_subset)\n",
    "                \n",
    "                cand_idx = trend_subset['TrendIndex'].idxmax() if find_max == True else trend_subset['TrendIndex'].idxmin()\n",
    "                cand_in_five = trend_subset.iloc[cand_idx][\"Keyword\"]  \n",
    "                cand_subset.append(cand_in_five)\n",
    "                print(cand_in_five, \" in \", movie_subset)\n",
    "            return(cand_subset)\n",
    "        \n",
    "        print(\"\\n============================== CANDIDATES ==============================\")\n",
    "        cand = _search(init_cand)\n",
    "        if len(cand) != 1:\n",
    "            return(self._recursive_search(init_cand[init_cand.isin([*cand])]))\n",
    "        else:\n",
    "            return(cand)\n",
    "\n",
    "    def get_trend(self, target, along_with = False):\n",
    "        \"\"\"\n",
    "        특정 영화의 TREND를 조회한다. along_with = True일 경우, min/max moive와 함께 조회하여 스케일을 조정한다.\n",
    "        \"\"\"\n",
    "        target_input = target\n",
    "        if along_with == True:\n",
    "            target = set([*target, *self.min_movie, *self.max_movie]) \n",
    "            print(\"Scale Info: max_movie: {0} | min_movie: {1} | target: {2}\".format(self.max_movie, self.min_movie, target))\n",
    "        body = self._define_body(set([*target]))\n",
    "        response_dict_subset = self._get_response_body(body)\n",
    "        response_subset = self._response_to_df(response_dict_subset)\n",
    "        target_trend = response_subset[response_subset.Keyword.isin([*target_input])].reset_index(drop=True)\n",
    "        return(target_trend)\n",
    "\n",
    "    def set_movie(self, max = [], min = []):\n",
    "        self.max_movie = self._recursive_search(self.movie_list_full, find_max = True) if max == True else max        \n",
    "        self.min_movie = self._recursive_search(self.movie_list_full, find_max = False) if min == True else min\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_info = {\n",
    "    \"client_id\": \"fe5kOiBVrMopPhGmaeeh\",\n",
    "    \"client_secret\": \"zauzLV_BPn\",\n",
    "    \"url\": \"https://openapi.naver.com/v1/datalab/search\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_info = {\n",
    "    \"client_id\": \"6GPEYsepjIil0OCSMnU5\",\n",
    "    \"client_secret\": \"QUd5qt5QU9\",\n",
    "    \"url\": \"https://openapi.naver.com/v1/datalab/search\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD FILE\n",
    "# mv2019 = pd.read_table(\"~/crawling/data/movie2019.dat\", sep = \"|\", header=None, names = [\"Title\", \"Year\", \"Month\", \"Day\"])\n",
    "# open_date = mv2019.Year.str.replace(\"년\", \"\") + \"-\" + mv2019.Month.str.replace(\"월\", \"\").str.zfill(2) + \"-\" + mv2019.Day.str.replace(\"일\", \"\").str.zfill(2)\n",
    "# mv2019[\"OpenDate\"] = [datetime.strptime(i, \"%Y-%m-%d\") for i in open_date]\n",
    "# mv2019 = mv2019.drop(['Year', 'Month', 'Day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv2019 = pd.read_table(\"~/MovieTrends/data/movie_list_200602.csv\", sep = \",\", encoding = \"EUC-KR\", header=0, names = [\"Title\", \"Subtitle\", \"TitleEn\", \"OpenDate\", \"Count\"])\n",
    "mv2019.OpenDate = pd.Series([pd.Timestamp(datetime.strptime(i, '%Y%m%d')) for i in map(str, mv2019.OpenDate)])\n",
    "mv2019 = mv2019[[\"Title\", \"OpenDate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = NaverTrend()\n",
    "NT.set_api_info(api_info)\n",
    "NT.load_data(mv2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT.set_movie(max = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_subset_list = [[*NT.movie_list_full[idx:idx+4]] for idx in [*range(0, len(NT.movie_list_full), 4)]]\n",
    "result_list = [NT.get_trend(target_subset, True) for target_subset in target_subset_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_full = pd.concat(result_list)\n",
    "result_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_merged = result_full.merge(NT.movie_data_full, how=\"left\", left_on=\"Keyword\", right_on=\"Title\").drop(\"Title\", 1)\n",
    "result_merged[\"D\"] = result_merged.Time - result_merged.OpenDate\n",
    "result_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_merged.to_csv(\"~/crawling/data/result_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc('font', family='NanumGothic')\n",
    "mpl.rc('figure', figsize=(15,10))\n",
    "mpl.rc('axes', unicode_minus=False)\n",
    "\n",
    "# ax = sns.lineplot(x='Time', y='Trendindex', hue='Keyword', data=result)\n",
    "ax = sns.lineplot(x='D', y='TrendIndex', hue='Keyword', data=result_merged, legend=False)\n",
    "\n",
    "plt.title('Line Graph by seaborn', fontsize=20)\n",
    "plt.ylabel('Trend', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.legend(fontsize=12, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Scaling Method: MaxAbsScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_minmax = NT.get_trend([\"백두산\", \"말모이\", \"시동\", \"82년생 김지영\", \"감쪽같은 그녀\"])\n",
    "result_min = NT.get_trend([\"감쪽같은 그녀\", \"말모이\", \"시동\", \"82년생 김지영\"])\n",
    "result_max = NT.get_trend([\"백두산\", \"말모이\", \"시동\", \"82년생 김지영\"])\n",
    "result_1 = NT.get_trend([\"백두산\", \"말모이\"])\n",
    "result_2 = NT.get_trend([\"백두산\", \"시동\"])\n",
    "result_3 = NT.get_trend([\"백두산\", \"82년생 김지영\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cknx1 = result_minmax[result_minmax.Keyword == \"말모이\"].merge(result_1, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "cknx2 = result_minmax[result_minmax.Keyword == \"시동\"].merge(result_2, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "cknx3 = result_minmax[result_minmax.Keyword == \"82년생 김지영\"].merge(result_3, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "sum(cknx1[\"TrendIndex_x\"] != cknx1[\"TrendIndex_y\"])\n",
    "sum(cknx2[\"TrendIndex_x\"] != cknx2[\"TrendIndex_y\"])\n",
    "sum(cknx3[\"TrendIndex_x\"] != cknx3[\"TrendIndex_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckx1 = result_max[result_max.Keyword == \"말모이\"].merge(result_1, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "ckx2 = result_max[result_max.Keyword == \"시동\"].merge(result_2, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "ckx3 = result_max[result_max.Keyword == \"82년생 김지영\"].merge(result_3, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "sum(ckx1[\"TrendIndex_x\"] != ckx1[\"TrendIndex_y\"])\n",
    "sum(ckx2[\"TrendIndex_x\"] != ckx2[\"TrendIndex_y\"])\n",
    "sum(ckx3[\"TrendIndex_x\"] != ckx3[\"TrendIndex_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckn1 = result_min[result_min.Keyword == \"말모이\"].merge(result_1, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "ckn2 = result_min[result_min.Keyword == \"시동\"].merge(result_2, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "ckn3 = result_min[result_min.Keyword == \"82년생 김지영\"].merge(result_3, how=\"left\", left_on=[\"Keyword\", \"Time\"], right_on=[\"Keyword\", \"Time\"])\n",
    "sum(ckn1[\"TrendIndex_x\"] != ckn1[\"TrendIndex_y\"])\n",
    "sum(ckn2[\"TrendIndex_x\"] != ckn2[\"TrendIndex_y\"])\n",
    "sum(ckn3[\"TrendIndex_x\"] != ckn3[\"TrendIndex_y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of API Calls Needed to Find max_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def n_call(n_item, cnt = 0):\n",
    "    temp = math.ceil(n_item/5)\n",
    "    cnt = cnt + temp\n",
    "    if temp == 1:\n",
    "        return(cnt)\n",
    "    else:\n",
    "        return(n_call(temp, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_call(len(NT.movie_list_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diff between the exact number of calls needed and approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "d = [random.randint(100000000, 1000000000) for _ in range(10000)]\n",
    "x = [n_call(i) for i in d]\n",
    "y = [i/4 for i in d]\n",
    "\n",
    "diff = [abs(i-j) for i, j in zip(x, y)]\n",
    "sum(diff)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 폰트 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "font_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for font in fm.fontManager.ttflist:\n",
    "    if 'Nanum' in font.name:\n",
    "        print(font.name, font.fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.FontProperties(fname=font_list[10]).get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.get_cachedir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm '/home/jupyter-tshahn2020/.cache/matplotlib/fontlist-v310.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 (miniconda)",
   "language": "python",
   "name": "python3-mc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}